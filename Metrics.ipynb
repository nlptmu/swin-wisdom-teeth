{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ece0dc-e8e4-4fec-bcb3-29fd9230db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelAccuracy,\n",
    "    MultilabelAUROC,\n",
    "    MultilabelAveragePrecision,\n",
    "    MultilabelExactMatch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37827d-dffb-4497-aa3c-0d84331c779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"Saved/preds_targets_probs_40e_10f.csv\")\n",
    "\n",
    "# parse string arrays\n",
    "def parse_array(s):\n",
    "    return np.fromstring(s.strip(\"[]\"), sep=\" \")\n",
    "\n",
    "preds = results_df[\"Predictions\"].apply(parse_array).to_list()\n",
    "targets = results_df[\"Targets\"].apply(parse_array).to_list()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "preds = np.stack(preds)\n",
    "targets = np.stack(targets).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae253180-7612-44c0-a8cf-81db00b1ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_PATH = \"Saved/preds_targets_probs_40e_10f.csv\"  # change if needed\n",
    "FOLD_COL = \"Fold\"\n",
    "PRED_COL = \"Predictions\"\n",
    "TARG_COL = \"Targets\"\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# If you have explicit names, set them here. Otherwise they will be auto-generated.\n",
    "TARGET_NAMES = [\"Right Upper\", \"Left Upper\", \"Left Lower\", \"Right Lower\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def parse_array(s: str) -> np.ndarray:\n",
    "    return np.fromstring(str(s).strip(\"[]\"), sep=\" \")\n",
    "\n",
    "def _compute_fold_metrics(preds_np: np.ndarray,\n",
    "                          targets_np: np.ndarray,\n",
    "                          threshold: float = 0.5) -> dict:\n",
    "\n",
    "    preds_t   = torch.tensor(preds_np, dtype=torch.float32)\n",
    "    targets_t = torch.tensor(targets_np, dtype=torch.long)\n",
    "    num_labels = preds_t.size(1)\n",
    "\n",
    "    subset_acc = MultilabelExactMatch(num_labels=num_labels)\n",
    "    macro_acc  = MultilabelAccuracy(num_labels=num_labels, average=\"macro\",    threshold=threshold)\n",
    "    per_label  = MultilabelAccuracy(num_labels=num_labels, average=None,       threshold=threshold)\n",
    "\n",
    "    auroc_ma   = MultilabelAUROC(num_labels=num_labels, average=\"macro\")\n",
    "    auprc_ma   = MultilabelAveragePrecision(num_labels=num_labels, average=\"macro\")\n",
    "\n",
    "    out = {\n",
    "        \"n\":             int(preds_t.size(0)),\n",
    "        \"subset_acc\":    float(subset_acc(preds_t, targets_t)),\n",
    "        \"macro_acc\":     float(macro_acc(preds_t, targets_t)),\n",
    "        \"macro_auroc\":   float(auroc_ma(preds_t, targets_t)),\n",
    "        \"macro_auprc\":   float(auprc_ma(preds_t, targets_t)),\n",
    "        \"per_label_acc\": per_label(preds_t, targets_t).detach().cpu().numpy(),  # shape (C,)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def compute_metrics_by_fold(results_df: pd.DataFrame,\n",
    "                            target_names=None,\n",
    "                            fold_col: str = \"Fold\",\n",
    "                            pred_col: str = \"Predictions\",\n",
    "                            targ_col: str = \"Targets\",\n",
    "                            threshold: float = 0.5):\n",
    "\n",
    "    first_row = results_df[pred_col].iloc[0]\n",
    "    C = parse_array(first_row).shape[0]\n",
    "    if target_names is None:\n",
    "        target_names = [f\"Label_{i}\" for i in range(C)]\n",
    "    else:\n",
    "        assert len(target_names) == C, f\"len(target_names)={len(target_names)} but C={C}\"\n",
    "\n",
    "    folds = sorted(results_df[fold_col].unique())\n",
    "    per_fold_rows, per_label_rows = [], []\n",
    "\n",
    "    for f in folds:\n",
    "        df_f = results_df[results_df[fold_col] == f]\n",
    "\n",
    "        preds_np   = np.stack(df_f[pred_col].apply(parse_array).to_list()).astype(np.float32)  # (N_f, C)\n",
    "        targets_np = np.stack(df_f[targ_col].apply(parse_array).to_list()).astype(np.int64)    # (N_f, C)\n",
    "\n",
    "        fm = _compute_fold_metrics(preds_np, targets_np, threshold=threshold)\n",
    "\n",
    "        row = {\"fold\": f, \"n\": fm[\"n\"]}\n",
    "        for k in [\"subset_acc\",\"macro_acc\",\n",
    "                 \"macro_auroc\",\"macro_auprc\"]:\n",
    "            row[k] = fm[k]\n",
    "        per_fold_rows.append(row)\n",
    "\n",
    "        # per-label accuracy for this fold\n",
    "        pla = fm[\"per_label_acc\"]\n",
    "        pla_row = {\"fold\": f, **{f\"acc_{name}\": float(pla[i]) for i, name in enumerate(target_names)}}\n",
    "        per_label_rows.append(pla_row)\n",
    "\n",
    "    per_fold_df = pd.DataFrame(per_fold_rows).set_index(\"fold\").sort_index()\n",
    "    per_label_acc_df = pd.DataFrame(per_label_rows).set_index(\"fold\").sort_index()\n",
    "    per_label_acc_df.loc[\"mean\"] = per_label_acc_df.mean(axis=0)\n",
    "\n",
    "    return per_fold_df, per_label_acc_df, target_names\n",
    "\n",
    "def summarize_across_folds(per_fold_df: pd.DataFrame):\n",
    "\n",
    "    if \"n\" not in per_fold_df.columns:\n",
    "        raise ValueError(\"per_fold_df must contain 'n'\")\n",
    "\n",
    "    totals = {\"n\": int(per_fold_df[\"n\"].sum())}\n",
    "    metric_cols = [c for c in per_fold_df.columns if c != \"n\"]\n",
    "\n",
    "    unweighted_mean = per_fold_df[metric_cols].mean().to_dict()\n",
    "    std_over_folds = per_fold_df[metric_cols].std(ddof=1).to_dict()\n",
    "\n",
    "    return unweighted_mean, totals, std_over_folds\n",
    "\n",
    "def pooled_metrics(results_df: pd.DataFrame,\n",
    "                   pred_col=\"Predictions\",\n",
    "                   targ_col=\"Targets\",\n",
    "                   threshold=0.5) -> dict:\n",
    "\n",
    "    preds_np = np.stack(results_df[pred_col].apply(parse_array).to_list()).astype(np.float32)\n",
    "    targs_np = np.stack(results_df[targ_col].apply(parse_array).to_list()).astype(np.int64)\n",
    "    return _compute_fold_metrics(preds_np, targs_np, threshold=threshold)\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load CSV\n",
    "    results_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    # Compute per-fold metrics\n",
    "    per_fold_df, per_label_acc_df, target_names = compute_metrics_by_fold(\n",
    "        results_df,\n",
    "        target_names=TARGET_NAMES,\n",
    "        fold_col=FOLD_COL,\n",
    "        pred_col=PREDCOL if (PREDCOL := PRED_COL) else \"Predictions\",  # defensive alias\n",
    "        targ_col=TARGCOL if (TARGCOL := TARG_COL) else \"Targets\",\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "\n",
    "    # Summaries across folds\n",
    "    unweighted_mean, totals, std_over_folds = summarize_across_folds(per_fold_df)\n",
    "\n",
    "    # Pooled metrics\n",
    "    pooled = pooled_metrics(results_df, pred_col=PRED_COL, targ_col=TARG_COL, threshold=THRESHOLD)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Print results\n",
    "    # ----------------------------\n",
    "    pd.set_option(\"display.width\", 180)\n",
    "    pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "    print(\"\\nPer-fold metrics:\")\n",
    "    print(per_fold_df.round(4))\n",
    "\n",
    "    print(\"\\nPer-label accuracy per fold:\")\n",
    "    print(per_label_acc_df.round(4))\n",
    "\n",
    "    print(f\"\\nTotal samples (sum over folds): {totals['n']}\")\n",
    "\n",
    "    print(\"\\nAverage over folds:\")\n",
    "    for k, v in unweighted_mean.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\nStd over folds:\")\n",
    "    for k, v in std_over_folds.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\nPooled over all folds:\")\n",
    "    for k, v in pooled.items():\n",
    "        if k == \"per_label_acc\":\n",
    "            continue\n",
    "        if k == \"n\":\n",
    "            print(f\"{k}: {int(v)}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-pytorch",
   "language": "python",
   "name": "env-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
