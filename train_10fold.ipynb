{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321b0be9-2bba-4435-83c4-2a89b6f8dffc",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a566c9-2334-495e-aa2a-5fc29f00c2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liewdj/anaconda3/envs/env-pytorch/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast \n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MultilabelAveragePrecision, MultilabelROC, MultilabelPrecisionRecallCurve, MultilabelConfusionMatrix, MultilabelHammingDistance\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score, ConfusionMatrix, Precision, MultilabelAUROC, MultilabelExactMatch\n",
    "from torchmetrics.regression import MeanAbsoluteError, R2Score, MeanSquaredError, MeanAbsoluteError\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.memory import garbage_collection_cuda\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, auc, average_precision_score, roc_auc_score, hamming_loss\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "import ast\n",
    "\n",
    "from progress_table import ProgressTable\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085987b-ea81-431a-810b-5691c60f05d9",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f687cf3c-0e9e-417e-aa32-c3426daa91e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f987eeb7410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 333\n",
    "\n",
    "def set_seed(seed: int = 333) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a56b4-894c-47b0-8dfb-a81720b91723",
   "metadata": {},
   "source": [
    "## Parameters/Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eae9952a-6193-468c-9296-4ed9a332d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"microsoft/swin-base-patch4-window12-384-in22k\"\n",
    "\n",
    "id2label = {0: \"Right Upper-0\", 1: \"Left Upper-1\", 2: \"Left Lower-2\", 3: \"Right Lower-3\"}\n",
    "label2id = {\"Right Upper-0\": 0, \"Left Upper-1\": 1, \"Left Lower-2\": 2, \"Right Lower-3\": 3}\n",
    "\n",
    "# training and model parameters\n",
    "patch_size = 16\n",
    "num_labels = 4\n",
    "num_heads = 4\n",
    "learn_rate = 7e-5\n",
    "steps_multipler = 1\n",
    "pct_start = 0.05\n",
    "num_folds = 10\n",
    "max_epoch = 40\n",
    "batch_size = 8\n",
    "accumulation_steps = 4\n",
    "drop = 0.2\n",
    "weight_decay = 0.01\n",
    "\n",
    "mmoe_params = {\n",
    "    'input_size': 3072,\n",
    "    'hidden_size': 768,\n",
    "    'compressed_size': 384,\n",
    "    'drop': 0.2,\n",
    "    'num_experts': 7,\n",
    "    'num_tasks': 4,\n",
    "    'tower_hidden_size': 768,\n",
    "    'num_heads': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceca490-d28d-4fe4-87ab-8505f6803ee6",
   "metadata": {},
   "source": [
    "## Load Original Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62192266-9995-4003-b2fe-98903e0d2b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Right Upper-0</th>\n",
       "      <th>Left Upper-1</th>\n",
       "      <th>Left Lower-2</th>\n",
       "      <th>Right Lower-3</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/6b65f571-1001_6.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/6f6d8ea6-1001_7.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/33795f3b-1001_8.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/eb160acb-1001_9.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/78741275-1001_10.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>images/891dfade-1129_991.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>images/b4fd6b9c-1129_992.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>images/8dd1c1e0-1129_1001.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>images/1f21c608-1129_1002.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>images/08b226b8-1129_1003.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5505 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ImagePath  Right Upper-0  Left Upper-1  \\\n",
       "0        images/6b65f571-1001_6.jpg              0             1   \n",
       "1        images/6f6d8ea6-1001_7.jpg              0             0   \n",
       "2        images/33795f3b-1001_8.jpg              0             0   \n",
       "3        images/eb160acb-1001_9.jpg              0             0   \n",
       "4       images/78741275-1001_10.jpg              0             0   \n",
       "...                             ...            ...           ...   \n",
       "5500   images/891dfade-1129_991.jpg              1             1   \n",
       "5501   images/b4fd6b9c-1129_992.jpg              1             1   \n",
       "5502  images/8dd1c1e0-1129_1001.jpg              0             0   \n",
       "5503  images/1f21c608-1129_1002.jpg              1             1   \n",
       "5504  images/08b226b8-1129_1003.jpg              0             0   \n",
       "\n",
       "      Left Lower-2  Right Lower-3        Labels  \n",
       "0                1              1  [0, 1, 1, 1]  \n",
       "1                0              0  [0, 0, 0, 0]  \n",
       "2                1              0  [0, 0, 1, 0]  \n",
       "3                1              1  [0, 0, 1, 1]  \n",
       "4                0              1  [0, 0, 0, 1]  \n",
       "...            ...            ...           ...  \n",
       "5500             1              1  [1, 1, 1, 1]  \n",
       "5501             0              0  [1, 1, 0, 0]  \n",
       "5502             1              0  [0, 0, 1, 0]  \n",
       "5503             1              1  [1, 1, 1, 1]  \n",
       "5504             0              0  [0, 0, 0, 0]  \n",
       "\n",
       "[5505 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"5505-Dataset.csv\")\n",
    "df['Labels'] = df['Labels'].apply(lambda x: ast.literal_eval(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c635f-fbb7-48cd-bada-4383db1a3731",
   "metadata": {},
   "source": [
    "## Dataset Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b582760-01b9-40fe-8ccb-6bcbc82c867d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "class DentalTrainDM(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.ImagePath = self.df[\"ImagePath\"]\n",
    "        self.Labels = self.df[\"Labels\"]\n",
    "        self.transform = transform\n",
    "        self.resize_transform = A.Compose([\n",
    "            A.Resize(height = 256, width = 256),\n",
    "            A.Normalize(normalization='min_max', mean=(0.5,), std=(0.5,), p = 1.0),\n",
    "            ToTensorV2(p = 1.0)\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.ImagePath[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Load as BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)   # Convert BGR to RGB\n",
    "\n",
    "        full_pixel_values = self.transform(image=image)['image'] # Apply transformations\n",
    "        \n",
    "        # Split the image into left and right halves\n",
    "        if len(full_pixel_values.shape) == 3:  # The transformed image has 3 dimensions (C, H, W)\n",
    "            height, width = full_pixel_values.shape[1], full_pixel_values.shape[2]\n",
    "            left_half = full_pixel_values[:, :, :width // 2]  # Left half\n",
    "            right_half = full_pixel_values[:, :, width // 2:]  # Right half\n",
    "        else:  # The transformed image is still 2D (H, W), no channel dimension\n",
    "            height, width = full_pixel_values.shape\n",
    "            left_half = full_pixel_values[:, :width // 2]  # Left half\n",
    "            right_half = full_pixel_values[:, width // 2:]  # Right half\n",
    "\n",
    "        Labels = torch.tensor(self.Labels[index], dtype=torch.long)\n",
    "    \n",
    "        return {\n",
    "            \"ImagePath\": image_path,\n",
    "            \"ImagePixels\": full_pixel_values,\n",
    "            \"LeftHalfPixels\": left_half,\n",
    "            \"RightHalfPixels\": right_half,\n",
    "            \"Labels\": Labels\n",
    "        }\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6ea8e39-17e7-4045-a607-1495aaf7b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DentalValDM(Dataset):\n",
    "    \n",
    "    def __init__(self, df, fold, transform):\n",
    "        self.df = df\n",
    "        self.ImagePath = self.df[\"ImagePath\"]\n",
    "        self.Labels = self.df[\"Labels\"]\n",
    "        self.transform = transform\n",
    "        self.fold = fold\n",
    "        self.resize_transform = A.Compose([\n",
    "            A.Resize(height = 256, width = 256),\n",
    "            A.Normalize(normalization='min_max', mean=(0.5,), std=(0.5,), p = 1.0),\n",
    "            ToTensorV2(p = 1.0)\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        fold_id = self.fold\n",
    "        image_path = self.ImagePath[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Load as BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    # Convert BGR to RGB\n",
    "\n",
    "        # Apply transformations\n",
    "        full_pixel_values = self.transform(image=image)['image']\n",
    "        resized_full_pixel_values = self.resize_transform(image=image)['image']\n",
    "\n",
    "        # Split the image into left and right halves\n",
    "        if len(full_pixel_values.shape) == 3:  # The transformed image has 3 dimensions (C, H, W)\n",
    "            height, width = full_pixel_values.shape[1], full_pixel_values.shape[2]\n",
    "            left_half = full_pixel_values[:, :, :width // 2]  # Left half\n",
    "            right_half = full_pixel_values[:, :, width // 2:]  # Right half\n",
    "        else:  # The transformed image is still 2D (H, W), no channel dimension\n",
    "            height, width = full_pixel_values.shape\n",
    "            left_half = full_pixel_values[:, :width // 2]  # Left half\n",
    "            right_half = full_pixel_values[:, width // 2:]  # Right half\n",
    "        \n",
    "        Labels = torch.tensor(self.Labels[index], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"Fold_ID\": fold_id,\n",
    "            \"ImagePath\": image_path,\n",
    "            \"ImagePixels\": full_pixel_values,\n",
    "            \"LeftHalfPixels\": left_half,\n",
    "            \"RightHalfPixels\": right_half,\n",
    "            \"Labels\": Labels\n",
    "        }\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8fbe3-7f26-43e4-8f7e-0c9cafa39bef",
   "metadata": {},
   "source": [
    "## Transforms/Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d462e16a-19ce-4422-9187-9f44200a983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training augmentation\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height = 512, width = 1024, p = 1.0),\n",
    "    A.ShiftScaleRotate(shift_limit =  (-0.02, 0.02), \n",
    "                       scale_limit = (-0, 0),  \n",
    "                       rotate_limit = (2),  \n",
    "                       interpolation = 1,  \n",
    "                       border_mode = cv2.BORDER_CONSTANT,\n",
    "                       # border_mode = 1,\n",
    "                       value = [232, 232, 232],  \n",
    "                       mask_value = 0, \n",
    "                       shift_limit_x = (-0, 0),  \n",
    "                       shift_limit_y = (-0.02, 0.02), \n",
    "                       rotate_method = \"largest_box\", \n",
    "                       always_apply = False,  \n",
    "                       p = 0.6),\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p = 0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit = (-0.1, 0.2), contrast_limit = (-0.1, 0.1), p = 0.3),\n",
    "    A.Blur(blur_limit = 3, p = 0.3),\n",
    "    A.Normalize(normalization='min_max', mean=(0.5,), std=(0.5,), p = 1.0),\n",
    "    ToTensorV2(p = 1.0)\n",
    "])\n",
    "\n",
    "# validation augmentation\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height = 512, width = 1024),\n",
    "    A.Normalize(normalization='min_max', mean=(0.5,), std=(0.5,), p = 1.0),\n",
    "    ToTensorV2(p = 1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89d5b6-6285-4807-9d93-a1e40274c934",
   "metadata": {},
   "source": [
    "## NN.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07489ac4-a003-4ead-b7e7-14c98febca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 towers: Right Upper, Left Upper, Left Lower, Right Lower\n",
    "class Right_Upper_0(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, output_size=1):\n",
    "        super(Right_Upper_0, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads = num_heads, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Classifier = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Classifier(x)  \n",
    "        return {\"logits\": x}\n",
    "\n",
    "class Left_Upper_1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, output_size=1):\n",
    "        super(Left_Upper_1, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Classifier = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Classifier(x)\n",
    "        return {\"logits\": x}\n",
    "\n",
    "class Left_Lower_2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, output_size=1):\n",
    "        super(Left_Lower_2, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Classifier = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Classifier(x)\n",
    "        return {\"logits\": x}\n",
    "\n",
    "class Right_Lower_3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, output_size=1):\n",
    "        super(Right_Lower_3, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Classifier = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Classifier(x)\n",
    "        return {\"logits\": x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19548bc-25ac-4aac-92ab-11bcb208757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert Module\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, compressed_size, drop):\n",
    "        super(Expert, self).__init__()\n",
    "        self.compression = nn.Linear(input_size, compressed_size)\n",
    "        self.expansion = nn.Linear(compressed_size, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.final_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p = drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compression(x)\n",
    "        x = self.expansion(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Gate Module\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_experts):\n",
    "        super(Gate, self).__init__()\n",
    "        self.Linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.Linear2 = nn.Linear(hidden_size, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "# MMoE Module\n",
    "class MMoE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, compressed_size, drop, num_experts, num_tasks, tower_hidden_size, num_heads):\n",
    "        super(MMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_size, hidden_size, compressed_size, drop) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gates\n",
    "        self.gates = nn.ModuleList([\n",
    "            Gate(input_size, hidden_size, num_experts) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "        self.right_upper_tower = Right_Upper_0(hidden_size, tower_hidden_size, num_heads)\n",
    "        self.left_upper_tower = Left_Upper_1(hidden_size, tower_hidden_size, num_heads)\n",
    "        self.left_lower_tower = Left_Lower_2(hidden_size, tower_hidden_size, num_heads)\n",
    "        self.right_lower_tower = Right_Lower_3(hidden_size, tower_hidden_size, num_heads)\n",
    "        \n",
    "        # Task-specific Towers\n",
    "        self.towers = nn.ModuleList([\n",
    "            self.right_upper_tower,  \n",
    "            self.left_upper_tower,  \n",
    "            self.left_lower_tower,\n",
    "            self.right_lower_tower\n",
    "        ])\n",
    "    \n",
    "    def forward(self, last_hidden_state):\n",
    "        # Pass full sequence to Experts\n",
    "        expert_outputs = torch.stack([expert(last_hidden_state) for expert in self.experts], dim=2)\n",
    "      \n",
    "        # Extract CLS token for Gates\n",
    "        cls_token = last_hidden_state\n",
    "\n",
    "        # Map tower names to their indices\n",
    "        tower_names = [\"Right-Upper\", \"Left-Upper\", \"Left-Lower\", \"Right-Lower\"]\n",
    "        \n",
    "        task_outputs = {}\n",
    "        for i, (gate, tower_name) in enumerate(zip(self.gates, tower_names)):\n",
    "\n",
    "            # Gate Output\n",
    "            gate_output = gate(cls_token)\n",
    "            \n",
    "            # Unsqueeze Gate Output for Weighted Sum Computation\n",
    "            gate_output_unsqueeze = gate_output.unsqueeze(1)\n",
    "            \n",
    "            # Broadcast Gate Output\n",
    "            gate_output_broadcasted = gate_output_unsqueeze.expand(-1, expert_outputs.size(1), -1)\n",
    "            \n",
    "            # Weighted Sum of Expert Outputs\n",
    "            weighted_expert_output = torch.einsum(\n",
    "                \"bne,bne->bn\", gate_output_broadcasted, expert_outputs\n",
    "            )\n",
    "            \n",
    "            # Pass through the Task-Specific Tower\n",
    "            tower_output = self.towers[i](weighted_expert_output)\n",
    "            task_outputs[tower_name] = tower_output\n",
    "    \n",
    "        return task_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d259ae6-7e50-48c4-859a-6ed3fa64c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_features, out_features * 2)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return self.dropout(x * F.gelu(gate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb172305-55bf-434c-9de5-d11c80c98220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DentalTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_type, num_class = 4, dropout = 0.2, layer_start = 4, hidden_size = 1024):\n",
    "        super(DentalTransformer, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_type, output_attentions = True, output_hidden_states = True, \n",
    "                                              id2label = id2label, label2id = label2id, image_size = (512, 1024))\n",
    "\n",
    "        self.mmoe = MMoE(**mmoe_params) \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            GEGLU(3072, 1536),  # GEGLU layer instead of Linear(3072, 1536) + GELU\n",
    "            nn.Dropout(p=dropout),\n",
    "            GEGLU(1536, 768),   # GEGLU layer instead of Linear(1536, 768) + GELU\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(768, num_class)  # Output layer remains the same\n",
    "        )\n",
    "        \n",
    "    def forward(self, full_pixels, left_pixels, right_pixels, interpolate_pos_encoding=True):\n",
    "        \n",
    "        main_re_hidden = self.model(full_pixels)[\"reshaped_hidden_states\"][4]\n",
    "        left_re_hidden = self.model(left_pixels)[\"reshaped_hidden_states\"][4]\n",
    "        right_re_hidden = self.model(right_pixels)[\"reshaped_hidden_states\"][4]\n",
    "\n",
    "        full_pool = main_re_hidden.mean(dim=(-2, -1))   # Shape: [B, 1024]\n",
    "        left_pool = left_re_hidden.mean(dim=(-2, -1))     # Shape: [B, 1024]\n",
    "        right_pool = right_re_hidden.mean(dim=(-2, -1))   # Shape: [B, 1024]\n",
    "\n",
    "        concat_pool = torch.cat((left_pool, full_pool, right_pool), dim=1)\n",
    "        # print(concat_pool.shape)\n",
    "\n",
    "        mmoe_out = self.mmoe(concat_pool)\n",
    "        # print(mmoe_out)\n",
    "        \n",
    "        # outputs = self.classifier(concat_pool)  \n",
    "        \n",
    "        return mmoe_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db6ca5-2169-4fb9-84b7-0de3484853c9",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feabcddb-2b3d-484b-99a7-af36096fe08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(outputs, targets):\n",
    "    targets = targets.float()\n",
    "    loss = nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbfb32-71d3-409e-8267-fe07e186dc5a",
   "metadata": {},
   "source": [
    "## Training & Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eea14ac-11a8-433c-bb77-c9d588b2b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ProgressTable(num_decimal_places=4, interactive=1)\n",
    "table.add_columns(\"Fd\", \"Ep\")\n",
    "table.add_columns(\"T Loss\")\n",
    "table.add_columns(\"T Acc\", \"T F1\", \"T Ham\")  # Classification metrics\n",
    "table.add_columns(\"T Steps\")\n",
    "table.add_columns(\"Time/Ep\")\n",
    "table.add_columns(\"V Loss\")\n",
    "table.add_columns(\"V SubAcc\", \"V Pre\", \"V Rec\", \"V F1\", \"V Ham\", \"V MacroAcc\")          # Validation classification metrics\n",
    "table.add_columns(\"V Steps\")\n",
    "table.add_columns(\"BestEpoch\")\n",
    "table.add_columns(\"Learning Rate\")\n",
    "\n",
    "def train_model(start_epoch, n_epochs, valid_acc_max_input, training_loader, validation_loader, model, \n",
    "                optimizer, scheduler = None, checkpoint_path = None, best_model_path = None, run_id = None, continue_train = False, \n",
    "                model_state_dict = None, optimizer_state_dict = None, accumulation_steps = 2):\n",
    "    \n",
    "    fold = run_id+1\n",
    "\n",
    "    if continue_train == True:\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_acc_max = valid_acc_max_input \n",
    "    \n",
    "    # initialize metrics\n",
    "    t_sub_acc = MultilabelExactMatch(num_labels = 4).to(device)\n",
    "    t_ham_mi = MultilabelHammingDistance(num_labels = 4 , average='micro').to(device)\n",
    "\n",
    "    v_sub_acc = MultilabelExactMatch(num_labels = 4).to(device)\n",
    "    v_ham_mi = MultilabelHammingDistance(num_labels = 4 , average='micro').to(device)\n",
    "    v_macro_acc = MultilabelAccuracy(num_labels = 4, average = \"macro\").to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs+1):\n",
    "\n",
    "        # Reset metrics\n",
    "        t_sub_acc.reset()\n",
    "        t_ham_mi.reset()\n",
    "\n",
    "        v_sub_acc.reset()\n",
    "        v_ham_mi.reset()\n",
    "        v_macro_acc.reset()\n",
    "        \n",
    "        t_epoch_f1 = []\n",
    "\n",
    "        v_epoch_f1 = []\n",
    "        v_epoch_precision = []\n",
    "        v_epoch_recall = []\n",
    "        \n",
    "        table.update(\"Ep\", f\"{epoch}/{n_epochs}\")\n",
    "        table.update(\"Fd\", fold)\n",
    "        e_t0 = time.time()\n",
    "        \n",
    "        # step counter\n",
    "        t_steps = 0\n",
    "        v_steps = 0\n",
    "        \n",
    "        t_f1_cumulative = 0\n",
    "        v_f1_cumulative = 0\n",
    "        v_precision_cumulative = 0\n",
    "        v_recall_cumulative = 0\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        for i, data in enumerate(training_loader, 0):\n",
    "            \n",
    "            t_full_pixels = data[\"ImagePixels\"].to(device, non_blocking=True)\n",
    "            t_left_pixels = data[\"LeftHalfPixels\"].to(device, non_blocking=True)\n",
    "            t_right_pixels = data[\"RightHalfPixels\"].to(device, non_blocking=True)\n",
    "            t_class_labels = data[\"Labels\"].to(device, dtype = torch.long, non_blocking=True)\n",
    "\n",
    "            if i % accumulation_steps == 0:\n",
    "                model.zero_grad(set_to_none=True)\n",
    "\n",
    "                # training step\n",
    "                t_steps += 1\n",
    "                table.update(\"T Steps\", f\"{t_steps}/{int((len(training_loader) / accumulation_steps))}\") # f\"{t_progress:.2%}\"   \n",
    "                \n",
    "            with torch.autocast(\"cuda\"):\n",
    "\n",
    "                outputs = model(full_pixels = t_full_pixels, left_pixels = t_left_pixels, right_pixels = t_right_pixels)\n",
    "                t_class_logits = torch.cat((outputs[\"Right-Upper\"][\"logits\"], outputs[\"Left-Upper\"][\"logits\"], outputs[\"Left-Lower\"][\"logits\"], outputs[\"Right-Lower\"][\"logits\"]), 1)\n",
    "                t_class_preds = torch.sigmoid(t_class_logits)\n",
    "\n",
    "                t_class_loss = bce_loss(t_class_logits, t_class_labels)                 \n",
    "                table.update(\"T Loss\", f\"{t_class_loss.item():.4f}\", aggregate = \"mean\")\n",
    "            \n",
    "            # torch metrics\n",
    "            t_sub_acc.update(t_class_preds, t_class_labels) \n",
    "            t_acc = t_sub_acc.compute()\n",
    "            table.update(\"T Acc\", f\"{t_acc.item():.4f}\", aggregate = \"mean\")\n",
    "\n",
    "            t_ham_mi.update(t_class_preds, t_class_labels)\n",
    "            t_ham = t_ham_mi.compute()\n",
    "            table.update(\"T Ham\", f\"{t_ham.item():.4f}\", aggregate = \"mean\")\n",
    "\n",
    "            # sklearn metrics\n",
    "            t_preds_np = (t_class_preds.detach().cpu().numpy() >= 0.5).astype(int)\n",
    "            t_labels_np = t_class_labels.detach().cpu().numpy()\n",
    "                \n",
    "            t_f1 = f1_score(t_labels_np, t_preds_np, average=\"samples\")\n",
    "            t_f1_cumulative += t_f1\n",
    "            t_f1_running_avg = t_f1_cumulative / (i + 1)\n",
    "            table.update(\"T F1\", f\"{t_f1_running_avg:.4f}\", aggregate = \"mean\")\n",
    "            t_epoch_f1.append(t_f1)\n",
    "            \n",
    "            # backprop\n",
    "            t_class_loss = t_class_loss / accumulation_steps\n",
    "            t_class_loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            if i % accumulation_steps == accumulation_steps - 1 or (i + 1) == len(training_loader):\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    if scheduler.last_epoch < scheduler.total_steps:  # Prevent stepping beyond total_steps\n",
    "                        scheduler.step()\n",
    "                    \n",
    "                opt_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "                table.update(\"Learning Rate\", f\"{opt_lr:.8f}\")\n",
    "        \n",
    "        # epoch time\n",
    "        epoch_time = time.time() - e_t0\n",
    "        table.update(\"Time/Ep\",f\"{epoch_time:.2f}\")\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, data in enumerate(validation_loader, 0):\n",
    "                    \n",
    "                v_full_pixels = data[\"ImagePixels\"].to(device, non_blocking=True)\n",
    "                v_left_pixels = data[\"LeftHalfPixels\"].to(device, non_blocking=True)\n",
    "                v_right_pixels = data[\"RightHalfPixels\"].to(device, non_blocking=True)\n",
    "                v_class_labels = data[\"Labels\"].to(device, dtype = torch.long, non_blocking=True)      \n",
    "                    \n",
    "                with torch.autocast(\"cuda\"):\n",
    "    \n",
    "                    outputs = model(full_pixels = v_full_pixels, left_pixels = v_left_pixels, right_pixels = v_right_pixels)\n",
    "                    v_class_logits = torch.cat((outputs[\"Right-Upper\"][\"logits\"], outputs[\"Left-Upper\"][\"logits\"], outputs[\"Left-Lower\"][\"logits\"], outputs[\"Right-Lower\"][\"logits\"]), 1)\n",
    "                    v_class_preds = torch.sigmoid(v_class_logits)\n",
    "    \n",
    "                    v_class_loss = bce_loss(v_class_logits, v_class_labels)\n",
    "                    table.update(\"V Loss\", f\"{v_class_loss.item():.4f}\", aggregate = \"mean\")\n",
    "                    \n",
    "                # validation loss/step \n",
    "                v_steps += 1   \n",
    "                table.update(\"V Steps\", f\"{v_steps}/{(len(validation_loader))}\") # f\"{v_progress:.2%}\"\n",
    "                                  \n",
    "                # torch metrics\n",
    "                v_sub_acc.update(v_class_preds, v_class_labels) \n",
    "                v_acc = v_sub_acc.compute()\n",
    "                table.update(\"V SubAcc\", f\"{v_acc:.4f}\", aggregate = \"mean\")\n",
    "\n",
    "                v_ham_mi.update(v_class_preds, v_class_labels)\n",
    "                v_ham = v_ham_mi.compute()\n",
    "                table.update(\"V Ham\", f\"{v_ham.item():.4f}\", aggregate = \"mean\")\n",
    "\n",
    "                v_macro_acc.update(v_class_preds, v_class_labels)\n",
    "                v_ma_acc = v_macro_acc.compute()\n",
    "                table.update(\"V MacroAcc\", f\"{v_ma_acc.item():.4f}\", aggregate = \"mean\")\n",
    "\n",
    "                # sklearn metrics\n",
    "                v_preds_np = (v_class_preds.detach().cpu().numpy() >= 0.5).astype(int)\n",
    "                v_labels_np = v_class_labels.detach().cpu().numpy()\n",
    "                \n",
    "                v_f1 = f1_score(v_labels_np, v_preds_np, average=\"samples\")\n",
    "                v_f1_cumulative += v_f1\n",
    "                v_f1_running_avg = v_f1_cumulative / (i + 1)\n",
    "                table.update(\"V F1\", f\"{v_f1_running_avg:.4f}\", aggregate = \"mean\")\n",
    "                v_epoch_f1.append(v_f1)\n",
    "                \n",
    "                v_precision = precision_score(v_labels_np, v_preds_np, average=\"samples\")\n",
    "                v_precision_cumulative += v_precision\n",
    "                v_precision_running_avg = v_precision_cumulative / (i + 1)\n",
    "                table.update(\"V Pre\", f\"{v_precision_running_avg:4f}\", aggregate = \"mean\")\n",
    "                v_epoch_precision.append(v_precision)\n",
    "                        \n",
    "                v_recall = recall_score(v_labels_np, v_preds_np, average=\"samples\")\n",
    "                v_recall_cumulative += v_recall\n",
    "                v_recall_running_avg = v_recall_cumulative / (i + 1)\n",
    "                table.update(\"V Rec\", v_recall_running_avg, aggregate = \"mean\")\n",
    "                v_epoch_recall.append(v_recall)                \n",
    "                \n",
    "        # save the model if validation f1 is higer than max\n",
    "        if valid_acc_max <= v_epoch_ma_acc.item():\n",
    "            torch.save(model.state_dict(), f'./Saved/f{fold}_best.pth')\n",
    "            best_epoch = epoch\n",
    "            valid_acc_max = v_epoch_ma_acc.item()\n",
    "        table.update(\"BestEpoch\", best_epoch)\n",
    "        \n",
    "        table.next_row()\n",
    "\n",
    "    del model\n",
    "    del training_loader\n",
    "    del validation_loader\n",
    "    collect_garbage()\n",
    "        \n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96164a9-1333-424e-bef6-78f2ec349669",
   "metadata": {},
   "source": [
    "## Load/Save checkpoint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "517a770b-4040-49f3-b2b6-e04ae3a62eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer = None):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "# save checkpoint\n",
    "def save_checkpoint(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86084d7-6c75-4e39-9fb3-969c40e10d4d",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c679c893-e7c1-4a37-bc28-abfbc9b6da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(best_model, validation_loader):\n",
    "    \n",
    "    best_model.eval()\n",
    "\n",
    "    imagepath_list = []\n",
    "    \n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    probs_list = []\n",
    "    \n",
    "    fold_list = []\n",
    "    \n",
    "    softmax = torch.nn.Softmax(dim = 1)\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            steps += 1\n",
    "\n",
    "            fold = data[\"Fold_ID\"]\n",
    "            imagepath = data[\"ImagePath\"]\n",
    "            full_pixels = data[\"ImagePixels\"].to(device)\n",
    "            left_pixels = data[\"LeftHalfPixels\"].to(device, non_blocking=True)\n",
    "            right_pixels = data[\"RightHalfPixels\"].to(device, non_blocking=True)\n",
    "            labels = data[\"Labels\"].to(device, dtype = torch.long)      \n",
    "            \n",
    "            outputs = best_model(full_pixels = full_pixels, left_pixels = left_pixels, right_pixels = right_pixels)\n",
    "            class_logits = torch.cat((outputs[\"Right-Upper\"][\"logits\"], outputs[\"Left-Upper\"][\"logits\"], outputs[\"Left-Lower\"][\"logits\"], outputs[\"Right-Lower\"][\"logits\"]), 1)\n",
    "            sigmoid_preds = torch.sigmoid(class_logits)\n",
    "\n",
    "            # Append classification outputs\n",
    "            imagepath_list.extend(imagepath)\n",
    "            preds_list.extend(sigmoid_preds)\n",
    "            targets_list.extend(labels)\n",
    "            fold_list.extend(fold)\n",
    "            \n",
    "    return preds_list, targets_list, imagepath_list, fold_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe88de-f433-438a-ae91-3d6f7dfc1a3a",
   "metadata": {},
   "source": [
    "## Compute Standard Deviation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc0aac97-15b0-4a4d-b213-6ea70bde3f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_std(metrics_df, max_epoch):\n",
    "    last_epoch_metrics = metrics_df.loc[(metrics_df['Epoch'] == f\"{max_epoch}/{max_epoch}\")]\n",
    "    print(\"Validation Metrics Standard Deviation\")\n",
    "    print(f\"Accuracy:  {np.std(last_epoch_metrics['V Acc'].to_list()):.4f}\")\n",
    "    print(f\"Precision: {np.std(last_epoch_metrics['V Precision'].to_list()):.4f}\")\n",
    "    print(f\"Recall:    {np.std(last_epoch_metrics['V Recall'].to_list()):.4f}\")\n",
    "    print(f\"F1 Score:  {np.std(last_epoch_metrics['V F1'].to_list()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8c8b2dd-b245-49a7-9f7f-b1f06f19ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "def collect_garbage():\n",
    "    garbage_collection_cuda()\n",
    "    time.sleep(3)\n",
    "    torch.cuda.empty_cache()\n",
    "    garbage_collection_cuda()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4e8fa-2c9f-4c98-9584-3672a1507862",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4017cd5-d4e1-4ae8-83ce-ebb35cfa96c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬────────────┬──────────┬───────────┬───────────────╮\n",
      "│    Fd    │    Ep    │  T Loss  │  T Acc   │   T F1   │  T Ham   │ T Steps  │ Time/Ep  │  V Loss  │ V SubAcc │  V Pre   │  V Rec   │   V F1   │  V Ham   │ V MacroAcc │ V Steps  │ BestEpoch │ Learning Rate │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼───────────┼───────────────┤\n",
      "│    1     │   1/40   │          │          │          │          │  1/155   │          │          │          │          │          │          │          │            │          │           │               │"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0408 22:05:28.858000 2607602 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "│    1     │   1/40   │  0.9095  │  0.3876  │  0.1205  │  0.3610  │ 156/155  │  603.14  │  0.2094  │  0.3575  │ 0.422727 │  0.6302  │  0.4827  │  0.3689  │   0.6311   │  69/69   │     1     │   0.00003683  │\n",
      "│    1     │   2/40   │  0.1865  │  0.6325  │  0.4555  │  0.1406  │ 156/155  │  515.21  │  0.6793  │  0.7514  │ 0.580616 │  0.6142  │  0.5881  │  0.0799  │   0.9201   │  69/69   │     2     │   0.00007000  │\n",
      "│    1     │   3/40   │  0.0098  │  0.7920  │  0.5595  │  0.0698  │ 156/155  │  518.09  │  0.0351  │  0.8094  │ 0.599832 │  0.6136  │  0.6001  │  0.0631  │   0.9369   │  69/69   │     3     │   0.00006988  │\n",
      "│    1     │   4/40   │  0.0261  │  0.8545  │  0.5865  │  0.0451  │ 156/155  │  506.77  │  0.0101  │  0.8457  │ 0.608825 │  0.5877  │  0.5926  │  0.0499  │   0.9501   │  69/69   │     4     │   0.00006951  │\n",
      "│    1     │   5/40   │  0.0015  │  0.8885  │  0.5988  │  0.0346  │ 156/155  │  504.04  │  0.4101  │  0.8802  │ 0.626812 │  0.6209  │  0.6195  │  0.0340  │   0.9660   │  69/69   │     5     │   0.00006891  │\n",
      "│    1     │   6/40   │  0.0203  │  0.9024  │  0.6070  │  0.0284  │ 156/155  │  501.67  │  0.0115  │  0.8893  │ 0.613138 │  0.6035  │  0.6042  │  0.0340  │   0.9660   │  69/69   │     6     │   0.00006807  │\n",
      "│    1     │   7/40   │  0.0065  │  0.9242  │  0.6119  │  0.0228  │ 156/155  │  509.95  │  0.0244  │  0.8693  │ 0.593383 │  0.5781  │  0.5815  │  0.0408  │   0.9592   │  69/69   │     6     │   0.00006701  │\n",
      "│    1     │   8/40   │  0.0057  │  0.9311  │  0.6180  │  0.0204  │ 156/155  │  500.89  │  0.0193  │  0.8947  │ 0.622110 │  0.6117  │  0.6132  │  0.0309  │   0.9691   │  69/69   │     8     │   0.00006572  │\n",
      "│    1     │   9/40   │  0.0016  │  0.9442  │  0.6214  │  0.0159  │ 156/155  │  495.72  │  0.1539  │  0.8947  │ 0.618034 │  0.6089  │  0.6101  │  0.0309  │   0.9691   │  69/69   │     8     │   0.00006422  │\n",
      "│    1     │  10/40   │  0.0034  │  0.9494  │  0.6233  │  0.0143  │ 156/155  │  489.93  │  0.2758  │  0.8947  │ 0.618595 │  0.6109  │  0.6105  │  0.0331  │   0.9669   │  69/69   │     8     │   0.00006252  │\n",
      "│    1     │  11/40   │  0.0004  │  0.9573  │  0.6254  │  0.0119  │ 156/155  │  487.63  │  0.0012  │  0.9165  │ 0.622110 │  0.6209  │  0.6184  │  0.0240  │   0.9760   │  69/69   │     11    │   0.00006063  │\n",
      "│    1     │  12/40   │  0.0014  │  0.9569  │  0.6272  │  0.0118  │  71/155  │          │          │          │          │          │          │          │            │          │           │   0.00005973  │"
     ]
    }
   ],
   "source": [
    "# Gather all predictions/targets for each fold\n",
    "all_preds_list = []\n",
    "all_targets_list = []\n",
    "all_probs_list = []\n",
    "all_path_list = []\n",
    "all_fold_list = []\n",
    "    \n",
    "skfold = MultilabelStratifiedKFold(n_splits = num_folds, random_state = seed, shuffle = True)\n",
    "y = np.array(df[\"Labels\"].tolist())\n",
    "for fold, (train_ids, val_ids) in enumerate(skfold.split(df, y)):\n",
    "    \n",
    "    train_dataset = df.loc[train_ids].copy().reset_index(drop = True)\n",
    "    val_dataset = df.loc[val_ids].copy().reset_index(drop = True)\n",
    "    \n",
    "    train_datamodule = DentalTrainDM(train_dataset, transform = train_transform)\n",
    "    val_datamodule = DentalValDM(val_dataset, fold = fold, transform = val_transform)\n",
    "    \n",
    "    \n",
    "    training_loader = torch.utils.data.DataLoader(train_datamodule, \n",
    "                                                  batch_size = batch_size, \n",
    "                                                  num_workers = 16,\n",
    "                                                  prefetch_factor = 2,\n",
    "                                                  pin_memory = True,\n",
    "                                                  shuffle = True)\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(val_datamodule,\n",
    "                                                    batch_size = batch_size, \n",
    "                                                    num_workers = 16,\n",
    "                                                    prefetch_factor = 2,\n",
    "                                                    pin_memory = True,\n",
    "                                                    shuffle = True\n",
    "                                                   )\n",
    "    \n",
    "    # initialize model\n",
    "    model = DentalTransformer(model_type, hidden_size = 768)\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "    model = torch.compile(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learn_rate, weight_decay = weight_decay)\n",
    "\n",
    "    batches_per_epoch = len(training_loader) \n",
    "    total_steps = int((batches_per_epoch / accumulation_steps) * max_epoch * steps_multipler)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = learn_rate, total_steps = total_steps, pct_start=pct_start)\n",
    "\n",
    "    # training\n",
    "    best_epoch = train_model(start_epoch = 1, n_epochs = max_epoch, valid_acc_max_input = 0,           \n",
    "                             training_loader = training_loader, validation_loader = validation_loader, \n",
    "                             model = model, optimizer = optimizer, scheduler = scheduler, run_id = fold, \n",
    "                             accumulation_steps = accumulation_steps)    \n",
    "    \n",
    "    # load maxepoch/best model\n",
    "    best_model_path = f'./Saved/f{fold+1}_best.pth'\n",
    "    best_model = load_checkpoint(best_model_path, model)\n",
    "    preds, targets, imagepath, run_id = prediction(best_model, validation_loader)\n",
    "\n",
    "    # collect all predictions and targets over all folds\n",
    "    all_preds_list.extend(preds)\n",
    "    all_targets_list.extend(targets)\n",
    "    all_path_list.extend(imagepath)\n",
    "    all_fold_list.extend(run_id)\n",
    "    \n",
    "    # compute single fold validation performance\n",
    "    targets_stacked = torch.stack(targets)  # Shape: [num_samples, num_labels]\n",
    "    preds_stacked = torch.stack(preds)  # Shape: [num_samples, num_labels]\n",
    "    \n",
    "    # Apply sigmoid and threshold to predictions\n",
    "    preds_probs = torch.sigmoid(preds_stacked)\n",
    "    preds_binary = (preds_probs >= 0.5).int()\n",
    "    \n",
    "    # Convert to NumPy\n",
    "    targets_np = targets_stacked.detach().cpu().numpy()\n",
    "    preds_np = preds_binary.detach().cpu().numpy()\n",
    "\n",
    "    performance = classification_report(targets_np, preds_np, \n",
    "                                        target_names = target_names, digits = 6, output_dict = True)\n",
    "    performance_report = pd.DataFrame(performance).transpose()\n",
    "    performance_report.to_csv(f'./Saved/BestPerformance_{version}_{architecture}_f{fold+1}.csv', index=True)\n",
    "\n",
    "    del model\n",
    "    del best_model\n",
    "    del training_loader\n",
    "    del validation_loader\n",
    "    collect_garbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f4a52-e4f6-41e9-a0ea-e029670aa4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.close()\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a92f0-dcba-4fae-9c82-5c8912cda2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = table.to_df()\n",
    "metrics_df.to_csv(f'Saved/{version}_{max_epoch}e_{fold+1}f_metrics.csv', index=False)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0f5e4a-08a9-4160-a6e1-15d35d487070",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_tar_df \u001b[38;5;241m=\u001b[39m  \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(all_fold_list), np\u001b[38;5;241m.\u001b[39marray(all_path_list), \n\u001b[1;32m      2\u001b[0m                                         torch\u001b[38;5;241m.\u001b[39mstack(all_preds_list)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \n\u001b[1;32m      3\u001b[0m                                         torch\u001b[38;5;241m.\u001b[39mstack(all_targets_list)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m                                        )\n\u001b[1;32m      5\u001b[0m                                    ),\n\u001b[1;32m      6\u001b[0m                                columns \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagePaths\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m pred_tar_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved/pred_tar_probs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchitecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mf.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m pred_tar_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pred_tar_df =  pd.DataFrame(list(zip(np.array(all_fold_list), np.array(all_path_list), \n",
    "                                        torch.stack(all_preds_list).detach().cpu().numpy(), \n",
    "                                        torch.stack(all_targets_list).detach().cpu().numpy()\n",
    "                                       )\n",
    "                                   ),\n",
    "                               columns =[\"Fold\", \"ImagePaths\", \"Predictions\", \"Targets\"])\n",
    "\n",
    "pred_tar_df.to_csv(f'Saved/preds_targets_probs_{max_epoch}e_{fold+1}f.csv', index=False)\n",
    "pred_tar_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-pytorch",
   "language": "python",
   "name": "env-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
